<html>
<head>
<title> Lecture notes - Chapter 13 - Performance features</title>
</head>

<BODY>
<h1> Chapter 13 -- performance features</h1>

<pre>


Architectural Features used to enhance performance
--------------------------------------------------

What is a "better" computer?  What is the "best" computer?
  The factors involved are generally cost and performance.

  COST FACTORS: cost of hardware design
		cost of software design (OS, applications)
		cost of manufacture
		cost to end purchaser
  PERFORMANCE FACTORS:
		what programs will be run?
		how frequently will they be run?
		how big are the programs?
		how many users?
		how sophisticated are the users?
		what I/O devices are necessary?


  (Chapter 13 discusses ways of increasing performance.)

There are two ways to make computers go faster.
 
 1. Wait a year.  Implement in a faster/better/newer technology.
    More transistors will fit on a single chip.
    More pins can be placed around the IC.
    The process used will have electronic devices (transistors)
      that switch faster.

 2. new/innovative architectures and architectural features.




MEMORY HIERARCHIES
------------------
Known in current technologies:  the time to access data
   from memory is an order of magnitude greater than a
   CPU operation.

   For example:  if a 32-bit 2's complement addition takes 1 time unit,
   then a load of a 32-bit word takes more than 10 time units.

Since every instruction takes at least one memory access (for
the instruction fetch), the performance of computer is dominated
by its memory access time.

  (to try to help this difficulty, we have load/store architectures,
   where most instructions take operands only from registers.  We also
   try to have fixed size, SMALL size, instructions.)


what we really want:
   very fast memory -- of the same speed as the CPU
   very large capacity -- 512 Mbytes
   low cost -- $50

   these are mutually incompatible.  The faster the memory,
   the more expensive it becomes.  The larger the amount of
   memory, the slower it becomes.

What we can do is to compromise.  Take advantage of the fact
(fact, by looking at many real programs) that memory accesses
are not random.  They tend to exhibit LOCALITY.

  LOCALITY -- something nearby.
  2 kinds:

  Locality in time (temporal locality)
    if data has been referenced recently, it is likely to
    be referenced again (soon!).

    example:  the instructions with in a loop.  The loop is
    likely to be executed more than once.  Therefore, each
    instruction gets referenced repeatedly in a short period
    of time.

    example: The top of stack is repeatedly referenced within
    a program.



  Locality in space (spacial locality)
    if data has been referenced recently, then data nearby
    (in memory) is likely to be referenced soon.

    example:  array access.  The elements of an array are
    neighbors in memory, and are likely to be referenced
    one after the other.

    example: instruction streams.  Instructions are located
    in memory next to each other.  Our model for program
    execution says that unless the PC is explicitly changed
    (like a branch or jump instruction) sequential instructions
    are fetched and executed. 


We can use these tendencies to advantage by keeping likely
to be referenced (soon) data in a faster memory than main
memory.  This faster memory is called a CACHE.


	CPU-cache   <----------------> memory


It is located very close to the processor.  The cache contains COPIES of
PARTS of memory.

A standard way of accessing memory, for a system with a cache:
 (The programmer doesn't see or know about any of this)
  
 Memory access is sent to the cache (first). It will be a READ or a WRITE,
   to accomplish an instruction fetch, a load, or a store.
 If the data is in the cache, then we have a HIT.
  For a read, the data is returned to the processor, and the memory access
  is completed.

 If the data is not in the cache, then we have a MISS.
   The memory access must be sent on to main memory.


 On average, the time to do a memory access for a machine with a
   single cache is

       AMAT = cache access time + (% misses  *  memory access time)

AMAT stands for Average Memory Access Time.

This average (mean) access time will change for each program.
It depends on the program, and its reference pattern, and how
that pattern interracts with the cache parameters.




A cache is managed by hardware.

	Keep recently-accessed blocks in the cache -- this exploits
	temporal locality.

	Break memory into aligned blocks (lines) of more than one
	word, to exploit spatial locality.

	Transfer data to/from the cache in blocks.

	put block in "block frame"
	  A simplified cache with 4 block frames may look like this:

	  ---------------------------------
	  |    one block fits here        |
	  ---------------------------------
	  |    a second block fits here   |
	  ---------------------------------
	  |    a third block fits here    |
	  ---------------------------------
	  |    a fourth block fits here   |
	  ---------------------------------


Any cache has far fewer block frames (places to put blocks) than  
   main memory has blocks.  So, we need a way of knowing which
   block from main memory goes in which block frame.

   A simple implementation uses part of the address  (remember that
   all memory accesses include an address) to map each block from main
   memory to a specific block frame in the cache.  In this way, the
   address can be used to determine the block frame that a block would
   be in (if the block is actually in the cache).

   Here is a simple diagram of which main memory block maps to which
   block frame, if there are just 4 block frames in the cache:


	    main memory blocks                        cache
	  --------------------                 --------------------
	  | maps to frame #1 |                 | frame #1         |
	  --------------------                 --------------------
	  | maps to frame #2 |                 | frame #2         |
	  --------------------                 --------------------
	  | maps to frame #3 |                 | frame #3         |
	  --------------------                 --------------------
	  | maps to frame #4 |                 | frame #4         |
	  --------------------                  --------------------
	  | maps to frame #1 |
	  --------------------
	  | maps to frame #2 |
	  --------------------
	  | maps to frame #3 |
	  --------------------
	  | maps to frame #4 |
	  --------------------
	  | maps to frame #1 |
	  --------------------
	  | maps to frame #2 |
	  --------------------
	  | maps to frame #3 |
	  --------------------
	        etc. (there are many more main memory blocks)



   For this simple example, where the cache has 4 block frames,
   we will need 2 bits of address as an INDEX # or LINE #.

   And, since a block contains more than one byte, and more than
   one word (to exploit spacial locality), we will need some of the
   bits from within the address to determine which byte or word
   of the block is desired.

   The address may be used by the cache as

             -------------------------------------------------
	     |   ?     |  INDEX #   | BYTE/WORD within BLOCK |
             -------------------------------------------------



   The remaining bits of the address will be used by the cache
   as a TAG.  Each main memory block (and cache block frame) has
   a TAG associated with it.
   The TAG distinguishes which main memory block is in a specific
   cache block frame.  This is necessary because many main memory blocks
   map to the same block frame (many main memory blocks have
   the same INDEX #).  The cache needs to know which one (of the
   many) is in the cache.

   The address is used by the cache as


             -------------------------------------------------
	     |  TAG    |  INDEX #   | BYTE/WORD within BLOCK |
             -------------------------------------------------


   Last item, then we have the whole picture.  It is possible
   (as when a program first starts), that nothing is in a cache's
   block frame.  The cache needs a way of distinguishing between
   an empty block frame and a full one.  A single bit per block
   called a VALID bit (or a PRESENT bit), determines if the block
   is currently empty or full.

   A diagram:

   address
   -------------------------------------------
   | TAG  | INDEX # | BYTE/WORD within BLOCK |
   -------------------------------------------
      |         |
      |         |     INDEX  VALID   TAG   DATA (BLOCK)
      |         |            ----------------------------------------
      |         |       00   |   |       |                          |
      |         |            ----------------------------------------
      |         |       01   |   |       |                          |
      |         |            ----------------------------------------
      |         ------> 10   |   |   .   |                          |
      |                      --------|-------------------------------
      |                 11   |   |   |   |                          |
      |                      --------|-------------------------------
      |                              |
      |                              |
      |    |--------------------------
      |    |
     \ /  \ /

     comparison




  Using this diagram, here's how the address and cache are used:

   The processor sends a memory access to the cache.  This access
     includes an address.
   The INDEX field within the address is used to determine the block
     frame of the data.
   The VALID bit of the correct block frame is checked to see if the
     frame actually contains data.
   If VALID bit active,  then the TAG of the block that is in the
     cache is checked to see if it matches the TAG within the address.
         If the TAGs match,
           then there is a HIT, and the least significant bits of the address
           are used to get the correct byte/word from within the block.

         If the TAGs do NOT match,
           then there is a MISS.

   If VALID bit is NOT active,  then there is no data in the block
     frame, and there is a MISS.
     
   In the case of a MISS, the memory access is sent to main
     memory, which responds by providing the block corresponding to
     the address.  The block is placed in the cache, the VALID bit set,
     the data is placed in the block frame, and the TAG of the block
     is placed in the TAG portion of the block frame.





An Unrealistic, Simplified Example to promote understanding of
 how a cache works:

   Addresses are 5 bits.
   Blocks are 4 bytes.
   Memory is byte addressable.
   There are 4 blocks in the cache.
   Assume that all accesses are to READ a single byte.

   Assume the cache is empty at the start of the example.

     (index #)
     (line #)     valid  tag  data (in hex)
       00           0     ?   0x?? ?? ?? ??
       01           0     ?   0x?? ?? ?? ??
       10           0     ?   0x?? ?? ?? ??
       11           0     ?   0x?? ?? ?? ??

   Memory is small enough that we can make up a complete
   example.  Assume little endian byte numbering.

     address   contents
     (binary)   (hex)
      00000    aa bb cc dd
      00100    00 11 22 33
      01000    ff ee 01 23
      01100    45 67 89 0a
      10000    bc de f0 1a
      10100    2a 3a 4a 5a
      11000    6a 7a 8a 9a
      11100    1b 2b 3b 4b

   (1)
   First memory reference is to the byte at address 01101.

   The address is broken into 3 fields:
     tag   index #       byte within block
      0        11             01

   On line 11, the block is marked as invalid, therefore
   we have a cache MISS.

   The block that address 01101 belongs to (4 bytes starting
   at address 01100) is brought into the cache, and the
   valid bit is set.

   (line number)  valid  tag  data (in hex)
       00           0     ?   0x?? ?? ?? ??
       01           0     ?   0x?? ?? ?? ??
       10           0     ?   0x?? ?? ?? ??
       11           1     0   0x45 67 89 0a

   And, now the data requested can be supplied to the processor.
   It is the value 0x89.



   (2)
   Second memory reference is to the byte at address 01010.

   The address is broken into 3 fields:
     tag   index #       byte within block
      0        10             10

   On line 10, the block is marked as invalid, therefore
   we have a cache MISS.

   The block that address 01010 belongs to (4 bytes starting
   at address 01000) is brought into the cache, and the
   valid bit is set.

   (line number)  valid  tag  data (in hex)
       00           0     ?   0x?? ?? ?? ??
       01           0     ?   0x?? ?? ?? ??
       10           1     0   0xff ee 01 23
       11           1     0   0x45 67 89 0a

   And, now the data requested can be supplied to the processor.
   It is the value 0xee.

   (3)
   Third memory reference is to the byte at address 01111.

   The address is broken into 3 fields:
     tag   index #       byte within block
      0        11             11

   This line within the cache has its valid bit set, so there
   is a block (from memory) in the cache.  BUT, is it the
   block that we want?  The tag of the desired byte is checked
   against the tag of the block currently in the cache.  They
   match, and therefore we have a HIT.

   The value 0x45 (byte 11 within the block) is supplied to
   the processor.



   (4)
   Fourth memory reference is to the byte at address 11010.

   The address is broken into 3 fields:
     tag   index #       byte within block
      1        10             10

   This line within the cache has its valid bit set, so there
   is a block (from memory) in the cache.  BUT, is it the
   block that we want?  The tag of the desired byte is checked
   against the tag of the block currently in the cache.  They
   do NOT match.  Therefore, the block currently in the cache
   is the wrong one.  It will be overwritten with the block
   (from memory) that we now do want.

   (line number)  valid  tag  data (in hex)
       00           0     ?   0x?? ?? ?? ??
       01           0     ?   0x?? ?? ?? ??
       10           1     1   0x6a 7a 8a 9a
       11           1     0   0x45 67 89 0a

   The value 0x7a (byte 10 within the block) is supplied to
   the processor.

   (5)
   Fifth memory reference is to the byte at address 11011.

   The address is broken into 3 fields:
     tag   index #       byte within block
      1        10             11

   This line within the cache has its valid bit set, so there
   is a block (from memory) in the cache.  BUT, is it the
   block that we want?  The tag of the desired byte is checked
   against the tag of the block currently in the cache.  They
   match, and therefore we have a HIT.

   The value 0x6a (byte 11 within the block) is supplied to
   the processor.


MISS RATIO =  fraction of total memory accesses that miss

HIT RATIO =  fraction of total memory accesses that hit = 1 - miss ratio

Often
	cache:  instruction cache 1 cycle
	        data cache 1 cycle
	main memory 20 cycles


 average memory access time
                  (AMAT) = cache-access + miss-ratio * memory-access
			 =       1     +   0.02     *  20
			 =       1.4



Beyond the scope of this class:
	block and block frames divided in "sets" (equivalence
	  classes) to speed lookup.
	terms: fully-associative, set-associative, direct-mapped



Typical cache size is 64K bytes, given a 64Mbyte memory
	cache is 20 times faster than main memory
	cache has 1/1000 the capacity of main memory
	cache often hits on 98% of the references!



Remember:

recently accessed blocks are in the cache (temporal locality)

the cache is smaller than main memory, so not all blocks are in the cache.

blocks are larger than 1 word (spacial locality)





This idea of exploiting locality is (can be) done at many
levels.  Implement a hierarchical memory system:

  smallest, fastest, most expensive memory         (registers)
  relatively small, fast, expensive memory         (CACHE)
  large, fast as possible, cheaper memory          (main memory)
  largest, slowest, cheapest (per bit) memory       (disk)



registers are managed/assigned by compiler or asm. lang programmer
cache is managed/assigned by hardware or partially by OS
main memory is managed/assigned by OS
disk managed by OS






This hierarchical scheme works so well (reducing the AMAT),
  that many systems now include 2 levels of caches!

  diagram

 -----------|----         ------        -------------
 |processor | L1|<------->| L2 |<------>|main memory|
 -----------|----         ------        -------------








Another common cache enhancement: use not one L1 cache,
but two special purpose caches.  One cache exclusively
holds instructions (code), and the other holds data (everything
else).

  Benefits:
    -- Each of the two caches can be designed to work in parallel.
    This means that both an instruction fetch memory request, and
    a data load/store can occur at the same time.  This overlap
    results in a performance improvement.
    -- The instruction cache (often called I-cache) can be relatively
    small, and still exhibit a rather high hit ratio.  When an entire
    loop's code fits into the cache, and the loop is executed many
    times, (after the initial misses that bring the code into the
    cache) the hit ratio is 1 (perfect).



Programmer's model:  one instruction is fetched and executed at
  a time.

Computer architect's model:  The effect of a program's execution are
  given by the programmer's model.  But, implementation may be
  different.

  To make execution of programs faster, we attempt to exploit
  PARALLELISM:  doing more than one thing at one time.

  Program level parallelism:  Have one program run parts of itself
    on more than one computer.  The different parts occasionally
    synch up (if needed), but they run at the same time.
  Instruction level parallelism (ILP):  Have more than one instruction
    within a single program executing at the same time.

PIPELINING  (ILP)
-----------------
 concept
 -------
   A task is broken down into steps.
   Assume that there are N steps, each takes the same amount of time.

   (Mark Hill's) EXAMPLE:  car wash

     steps:  P -- prep
	     W -- wash
	     R -- rinse
	     D -- dry
	     X -- wax

     assume each step takes 1 time unit

     SERIAL car wash:
     time to wash 1 car (red) = 5 time units
     time to wash 3 cars (red, green, blue) = 15 time units

     which car      time units
		1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
       red      P  W  R  D  X
       green                   P  W  R  D  X
       blue                                   P  W  R  D  X

   a PIPELINE overlaps the steps

     which car      time units
		1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
       red      P  W  R  D  X
       green       P  W  R  D  X
       blue           P  W  R  D  X
       yellow            P  W  R  D  X
	  etc.

	 IT STILL TAKES 5 TIME UNITS TO WASH 1 CAR,
	 BUT THE RATE OF CAR WASHES GOES UP!




   Pipelining can be done in computer hardware.

 2-stage pipeline
 ----------------
  steps:
    F -- instruction fetch (and PC update)
    E -- instruction execute (everything else)


    which instruction       time units
			1  2  3  4  5  6  7  8 . . .
       1                F  E
       2                   F  E
       3                      F  E
       4                         F  E

       
       time for 1 instruction =  2 time units
	 (INSTRUCTION LATENCY)

       rate of instruction execution = pipeline depth * (1 / time for     )
         (INSTRUCTION THROUGHPUT)                           1 instruction
				     =        2       * (1 /   2)
				     =   1 per time unit


 5-stage pipeline
 ----------------

 a popular pipelined implementation (used in MIPS R2000)
    (R6000 has 5 stages (but different), R4000 has 8 stages)

     steps:
	IF -- instruction fetch (and PC update)
	ID -- instruction decode (and get operands from registers)
	EX -- ALU operation (can be effective address calculation)
	MA -- memory access
	WB -- write back (results written to register(s))

    which       time units
instruction   1   2   3   4   5   6   7  8 . . .
     1        IF  ID  EX  MA  WB
     2            IF  ID  EX  MA  WB
     3                IF  ID  EX  MA  WB

    INSTRUCTION LATENCY = 5 time units
    INSTRUCTION THROUGHPUT = 5 * (1 / 5) = 1 instruction per time unit




unfortunately, pipelining introduces other difficulties. . .


 data dependencies
 -----------------

 suppose we have the following code:
   lw   $8, 4($sp)
   addi $9, $8, 1


   the data loaded doesn't get written to $8 until WB,
     but the addi instruction wants to get the data out of $8
     it its ID stage. . .

    which       time units
instruction   1   2   3   4   5   6   7  8 . . .
    lw        IF  ID  EX  MA  WB
			      ^^
    addi          IF  ID  EX  MA  WB
		      ^^
	
	the simplest solution is to STALL the pipeline.
	(Also called HOLES, HICCOUGHS or BUBBLES in the pipe.)

    which       time units
instruction   1   2   3   4   5   6   7   8   9 . . .
    lw        IF  ID  EX  MA  WB
			      ^^
    addi          IF  ID  ID  ID  ID  EX  MA  WB
		      ^^  ^^  ^^ (pipeline stalling)


   A DATA DEPENDENCY (also called a HAZARD) causes performance to
     decrease.  

The hardware that implements the stall essentially requires 1
bit per register.  This bit identifies if the register's contents
are not currently available for reading (due to an earlier instruction
not finished writing a value in that register).
  In the example, the ID stage of the lw instruction sets the bit for
  register $8, and the WB stage (or a counter of pipeline stages)
  resets the bit for $8.  The ID stage working on the addi instruction
  observes that register $8 is not available, and it stalls.  Each
  subsequent cycle, it checks again if register $8 is available.



 more on data dependencies
 -------------------------

   Read After Write (RAW) --
     (example given), a read of data is needed before it has been written

   Given for completeness, not a difficulty to current pipelines in
     practice, since the only writing occurs as the last stage.

         Write After Read (WAR) --
         Write After Write (WAR) --


   NOTE:  there is no difficulty implementing a 2-stage pipeline
   due to DATA dependencies!




 control dependencies
 --------------------

 what happens to a pipeline in the case of branch instructions?

 MAL CODE SEQUENCE:

        b  label1
        addi  $9, $8, 1
	sub   $10, $11, $12
	ori   $13, $14, 0xabcd
label1: mult $8, $9


AN INCORRECT SEQUENCE, TO ILLUSTRATE THE PROBLEM
    which       time units
instruction   1   2   3   4   5   6   7  8 . . .
     b        IF  ID  EX  MA  WB
			      ^^ (PC changed here)
    addi          IF  ID  EX  MA  WB
		  ^^  (WRONG instruction fetched here!)
    sub               IF  ID  EX  MA  WB
		      ^^  (another WRONG instruction fetched here!)
    ori                   IF  ID  EX  MA  WB
		          ^^  (another WRONG instruction fetched here!)
	

	Whenever the PC changes (except for PC <- PC + 4),
	we have a CONTROL DEPENDENCY.

	CONTROL DEPENDENCIES must be handled correctly by
	pipelines.  They cause performance to plummet.

	So, lots of (partial) solutions have been implemented
	to try to help the situation.
	  Worst case, the pipeline must be stalled such that
	  instructions are going through sequentially.

	Note that just stalling doesn't really help, since
	  the (potentially) wrong instruction is fetched
	  before it is determined that the previous instruction
	  is a branch.




BRANCHES and PIPELINING
-----------------------
 (Or, how to minimize the effect of control dependencies on pipelines.)

 easiest solution (poor performance)
    Cancel anything (later) in the pipe when a branch (jump) is decoded.
    This works as long as nothing changes the program's state
    before the cancellation.  Then let the branch instruction
    finish (flush the pipe), and start up again.

       which       time units
   instruction   1   2   3   4   5   6   7  8 . . .
        b        IF  ID  EX  MA  WB
			         ^^ (PC changed here)
       addi          IF
		     ^^ (cancelled) 
       mult                          IF  ID  EX  MA  WB
                                     (correct instruction fetched)

 branch Prediction (static or dynamic)
   add lots of extra hardware to try to help.

   a)  (static)  assume that the branch will not be taken
       When the decision is made, the hw "knows" if the correct
       instruction has been partially executed.

       If the correct instruction is currently in the pipe,
	 let it (and all those after it) continue.  Then,
	 there will be NO holes in the pipe.
       If the incorrect instruction is currently in the pipe,
	 (meaning that the branch was taken), then all instructions
	 currently in the pipe subsequent to the branch must
	 be BACKED OUT.
	 This can be done on the MIPS by disabling the WB stage
	 from writing any results back to registers.  This must
	 be done for all instructions that were incorrectly 
	 started.
       
   b)  (dynamic) A variation of (a).  
       Have some extra hw that keeps track of which branches have
       been taken in the recent past.  Design the hw to presume that
       a branch will be taken the same way it was previously.
       If the guess is wrong, back out as in (a).

       Question for the advanced student:  Which is better, (a) or (b)? Why?

   NOTE:  solution (a) works quite well with currently popular
      pipeline solutions, because no state information is changed
      until the very last stage of an instruction.  As long as
      the last stage hasn't started, backing out is a matter
      of stopping the last stage from occuring and getting the
      PC right.


Consider a generic sort of loop that we may write:


   loop:  b__  $?, $?, done_with_loop

          #instructions within the loop

	  b    loop    # unconditional branch to top

   done_with_loop:     # more code would go here


Assume that this loop is to be executed many times.


Static branch prediction can have excellent performance
with the conditional branch at the top of the loop.
It correctly predicts that the branch will not be taken
every time through the loop.  It predicts wrong the one
time that the loop is exited.

Static branch prediction does not help with the unconditional
branch at the bottom of the loop.  It will always predict incorrectly.

It is sometimes possible for a compiler generating assembly language
code to generate code that takes some of this into account,
setting up loops such that the common case is for a branch
to be NOT taken.




 separate test from branch
   make the conditional test and address calculation
   separate instructions from the one that changes the PC.

   This reduces the number of holes in the pipe.





 delayed branch
   MIPS solution.
   The concept:  prediction is always wrong sometime.
   There will be holes in the pipe when the prediction is wrong.
   So the goal is to reduce (eliminate?) the number of
   holes in the case of a branch.

   The mechanism:
     Have the effect of a branch (the change of the PC) be delayed
     until a subsequent instruction.  This means that the instruction
     following a branch is executed independent of whether the
     branch is to be taken or not.

     (NOTE: the simulator completely ignores this delayed branch
      mechanism!)

      code example, the programmer writes:
	
	  add $8, $9, $10
	  beq $3, $4,  label
	  move $18, $5
	  .
	  .
	  .
    label:  sub $20, $21, $22


Note that in this code example, we want one of two possibilities
for the code that gets executed:
 1) 
	  add $8, $9, $10
	  beq $3, $4,  label
	  move $18, $5
or 2)
	  add $8, $9, $10
	  beq $3, $4,  label
          sub $20, $21, $22

In both cases, the add and beq instructions are executed.

      This is turned into the following by a MIPS assembler:
	  add $8, $9, $10
	  beq $3, $4,  label
	  nop                  # really a pipeline hole, the DELAY SLOT
	  move $18, $5
	  .
	  .
	  .
    label:  sub $20, $21, $22



  If the assembler has any smarts at all, it would REARRANGE
  the code to be
	  beq $3, $4,  label
	  add $8, $9, $10
	  move $18, $5
	  .
	  .
	  .
    label:  sub $20, $21, $22

The ordering of the add and beq are changed, but the effect
is the same.  Both are executed.

  This code can be rearranged only if there are no data
  dependencies between the branch and the add instructions.
  In fact, any instruction from before the branch (and after any
  previous branch) can be moved into the DELAY SLOT, as long as
  there are no dependencies on it.


  Delayed branching depends on a smart assembler (SW) to make
  the hardware perform at peak efficiency.  This is a general
  trend in the field of computer science.  Let the SW do more
  and more to improve performance of the HW.


 squashing
   A fancy name for branch prediction that always presumes the
   branch will be taken,  and keeps a copy of the PC that will
   be needed in the case of backing out.



 condition codes
   a historically significant way of branching.  Condition codes
   are/were used on MANY machines before pipelining became popular.

   4 1-bit registers (condition code register):
     N -- negative
     V -- overflow
     P -- positive
     Z -- zero

  The result of an instruction set these 4 bits.
  Conditional branches were then based on these flags.

  Example:  bn label       # branch to label if the N bit is set

  Earlier computers had virtually every instruction set the
  condition codes.  This had the effect that the test (for
  the branch) needed to come directly before the branch.
  Example:  
	sub r3, r4, r5    # blt $4, $5, label 
	bn  label

  A performance improvement (sometimes) to this allowed the
  programmer to explicitly specify which instructions should
  set the condition codes.  In this way, (on a pipelined machine)
  the test could be separated from the branch, resulting in
  fewer pipeline holes due to data dependencies.





Superscalar processors
----------------------

We can make pipelining (parallelism at the instruction level)
one step better.

IF we have 2 instructions that contain no dependencies, then
both instructions could be executed at the same time without causing
any problems.

For example purposes, use the given 5-stage pipeline.
We could duplicate the hardware of each stage, adding extra to
check for dependencies.


  instr at PC       IF  ID  EX  MA  WB
  instr at PC+4     IF  ID  EX  MA  WB

  instr at PC+8         IF  ID  EX  MA  WB
  instr at PC+12        IF  ID  EX  MA  WB

  instr at PC+16            IF  ID  EX  MA  WB
  instr at PC+20            IF  ID  EX  MA  WB

This superscalar pipeline has twice the throughput of a pipeline
that issues a single instruction at one time.




Amdahl's Law
------------


We define the performance metric speedup:

speedup = new rate / old rate 

        = old execution time / new execution time

This metric compares the execution times of a specific program,
given specific input, before and after the code has been modified.
The modification makes the program better:  meaning the program
executes in less time after modification.

We enhance a portion of our program.
  Before modification, the fraction of total execution time spent in
  the portion of the program to be modified is f.

  The speedup of the modified part of the code is S.

  ( Let an enhancement speedup f fraction of the time by speedup S)

              [(1-f)+f]*old time
speedup = -----------------------------------
          (1-f) * old time + f/S * old time
  

	     1
	= ---------
	  1-f + f/S

Examples

	    f	    S		speedup
	   ---	   ---		-------
	   95%	   1.10		1.094
	    5%	   10		1.047
	    5%	   inf		1.052

An infinite speedup would mean that some portion of the
code was removed from the program.

lim		   1                 1
		---------	=  -----
S --> inf	1-f + f/S          1 - f
	
	 f	speedup
	---	-------
	1%      1.01
	2%      1.02
	5%      1.05
	10%     1.11
	20%     1.25
	50%     2.00

We must eliminate code that accounts for half the execution
time of the program in order to get a speedup of 2.

This says that we should concentrate on the common case!
Work on the code where it can do the most good.


</pre>
<HTML>
<HEAD>
<TITLE>CS/ECE 354 Spring 2000 (Section 2)</TITLE>
</HEAD>

<BODY> 
<pre>


A Bit more on Performance
---------------------------

To measure program performance, for a program that is executed:

 time         instrns     cycles     time
--------  =  --------- * -------- * -------
program       program     instrn     cycle


Instructions per program (path length)
      - number of instructions executed when the program runs
      - it depends on the architecture (instruction set) and the
        compiler used to produce the assembly language code

Cycles per instruction (CPI) = cycles / instrn

      -	it depends on the architecture and organization (like whether
        or not the implementation has a cache)

	SCPI = Stall Cycles Per Instruction
	(Useful for identifying impact of one component, such as cache misses)

	For data cache:
		SCPI = %load_store instr * %data_miss * stall_cycles_per_miss

	For instruction cache:
		SCPI = %instr_miss * stall_cycles_per_miss

	total CPI = CPI for perfect memory system 
			+ SCPI for data cache + SCPI for instruction cache


Time per cycle (cycle time, clock time)

        Organization and hardware
        (very hard to study in the abstract)

        Often a rate
	   100 MHz == 10 ns

	   five times faster clock:
	   100*5 MHz == 10/5 ns
	   500 MHz == 2 ns


Compare w/ Mhz only if
        instrns/program same (same instrn set)
        and CPI same (same implementation e.g., 386)

</pre>
<HR> <!-- ------------------------------------------------ -->

</BODY>
</HTML>

